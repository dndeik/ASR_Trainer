[network_config]


[GPUs]
devices = "0"   # number of available gpus

[optimizer]
lr = 0.0001

[model]
encoder_d_model = 640
predictor_d_model = 640
joiner_d_model = 640
layer_num = 18
n_heads = 10
n_groups = 5
chunk_size = 70
context_chunk_number = 4
mamba_every_n_block = -1  # if you don't want to use mamba, set -1
time_factor = 5
n_mel = 80

[tokenizer]
tokenizer_path = "models/ru_tokenizer/tokenizer_spe_bpe_cp"

[listener]
listener_sr = 16000

[FFT]
n_fft = 320
hop_length = 160
win_length = 320

[train_dataset]
train_manifest = "/opt/datasets/ASR_data/CAPITALIZED_TRAIN_FINAL.jsonl" # Put the path to your train dataset
file_num = -1
max_len = 13
min_len = 3

[train_dataloader]
batch_size = 10
num_workers = 6
drop_last = true
pin_memory = true

[validation_dataset]
train_manifest = "/opt/datasets/ASR_data/CAPITALIZED_TEST_FINAL.jsonl" # Put the path to your test dataset
file_num = -1
max_len = 18
min_len = 4

[validation_dataloader]
batch_size = 36
num_workers = 12
pin_memory = true

[logger]
log_to_clearml = false

[trainer]
epochs = 15
save_checkpoint_interval = 1
train_with_amp = true
grad_accum = 8
validation_interval = 1
clip_grad_norm_value = 3.0
exp_path = "experiments/gqa_with_mamba"
resume = false
resume_checkpoint_path = ''
resume_datetime = '2025-11-19-23h43m'
resume_step = 0

